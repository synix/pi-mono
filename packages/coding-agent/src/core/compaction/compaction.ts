/**
 * Context compaction for long sessions.
 *
 * Pure functions for compaction logic. The session manager handles I/O,
 * and after compaction the session is reloaded.
 */

import type { AgentMessage } from "@mariozechner/pi-agent-core";
import type { AssistantMessage, Model, Usage } from "@mariozechner/pi-ai";
import { completeSimple } from "@mariozechner/pi-ai";
import {
	convertToLlm,
	createBranchSummaryMessage,
	createCompactionSummaryMessage,
	createCustomMessage,
} from "../messages.js";
import type { CompactionEntry, SessionEntry } from "../session-manager.js";
import {
	computeFileLists,
	createFileOps,
	extractFileOpsFromMessage,
	type FileOperations,
	formatFileOperations,
	SUMMARIZATION_SYSTEM_PROMPT,
	serializeConversation,
} from "./utils.js";

// ============================================================================
// File Operation Tracking
// ============================================================================

/** Details stored in CompactionEntry.details for file tracking */
export interface CompactionDetails {
	readFiles: string[];
	modifiedFiles: string[];
}

// æ”¶é›†æ•´ä¸ªä¼šè¯ä¸­æ‰€æœ‰çš„æ–‡ä»¶æ“ä½œè®°å½•ï¼ˆå“ªäº›æ–‡ä»¶è¢«è¯»äº†ã€å“ªäº›è¢«ä¿®æ”¹äº†ï¼‰ï¼Œä¾› compaction æ‘˜è¦æœ«å°¾é™„åŠ æ–‡ä»¶åˆ—è¡¨ã€‚

/**
 * Extract file operations from messages and previous compaction entries.
 */
function extractFileOperations(
	messages: AgentMessage[],
	entries: SessionEntry[],
	prevCompactionIndex: number,
): FileOperations {
	const fileOps = createFileOps();

	/*
		ä»ä¸Šæ¬¡ compaction çš„ details ä¸­ç»§æ‰¿ï¼š
		å¦‚æœä¹‹å‰å·²ç»åšè¿‡ compactionï¼Œé‚£æ¬¡å‹ç¼©æ—¶è®°å½•çš„ readFiles å’Œ modifiedFiles ä¼šè¢«å–å‡ºæ¥ä½œä¸ºåŸºç¡€ã€‚
		è¿™æ ·æ–‡ä»¶æ“ä½œè®°å½•æ˜¯è·¨å¤šæ¬¡ compaction ç´¯ç§¯çš„ï¼Œä¸ä¼šå› ä¸ºå‹ç¼©è€Œä¸¢å¤±ã€‚(fromHookçš„æ£€æŸ¥æ˜¯è·³è¿‡å¤–éƒ¨ hook ç”Ÿæˆçš„ compactionï¼Œåªä¿¡ä»»è‡ªå·±ç”Ÿæˆçš„ã€‚)
	*/

	// Collect from previous compaction's details (if pi-generated)
	if (prevCompactionIndex >= 0) {
		const prevCompaction = entries[prevCompactionIndex] as CompactionEntry;
		if (!prevCompaction.fromHook && prevCompaction.details) {
			// fromHook field kept for session file compatibility
			const details = prevCompaction.details as CompactionDetails;
			if (Array.isArray(details.readFiles)) {
				for (const f of details.readFiles) fileOps.read.add(f);
			}
			if (Array.isArray(details.modifiedFiles)) {
				for (const f of details.modifiedFiles) fileOps.edited.add(f);
			}
		}
	}

	/*
		ä»å½“å‰å¾…å‹ç¼©çš„æ¶ˆæ¯ä¸­æå–ï¼šéå†æ‰€æœ‰æ¶ˆæ¯ï¼Œè°ƒç”¨ extractFileOpsFromMessage è§£æ tool call ä¸­çš„æ–‡ä»¶æ“ä½œ
		(æ¯”å¦‚ Readã€Editã€Writeç­‰å·¥å…·è°ƒç”¨çš„å‚æ•°é‡Œä¼šåŒ…å«æ–‡ä»¶è·¯å¾„ï¼‰ã€‚
	*/

	// Extract from tool calls in messages
	for (const msg of messages) {
		extractFileOpsFromMessage(msg, fileOps);
	}

	return fileOps;
}

// ============================================================================
// Message Extraction
// ============================================================================

/**
 * Extract AgentMessage from an entry if it produces one.
 * Returns undefined for entries that don't contribute to LLM context.
 */
function getMessageFromEntry(entry: SessionEntry): AgentMessage | undefined {
	if (entry.type === "message") {
		return entry.message;
	}
	if (entry.type === "custom_message") {
		return createCustomMessage(entry.customType, entry.content, entry.display, entry.details, entry.timestamp);
	}
	if (entry.type === "branch_summary") {
		return createBranchSummaryMessage(entry.summary, entry.fromId, entry.timestamp);
	}
	if (entry.type === "compaction") {
		return createCompactionSummaryMessage(entry.summary, entry.tokensBefore, entry.timestamp);
	}
	return undefined;
}

/** Result from compact() - SessionManager adds uuid/parentUuid when saving */
export interface CompactionResult<T = unknown> {
	summary: string;
	firstKeptEntryId: string;
	tokensBefore: number;
	/** Extension-specific data (e.g., ArtifactIndex, version markers for structured compaction) */
	details?: T;
}

// ============================================================================
// Types
// ============================================================================

/*
  3ä¸ªå­—æ®µçš„å«ä¹‰:

  1. enabled â€” æ˜¯å¦å¼€å¯ compactionï¼ˆä¸Šä¸‹æ–‡å‹ç¼©ï¼‰ã€‚å…³é—­åå¯¹è¯ä¸ä¼šè¢«è‡ªåŠ¨æ‘˜è¦å‹ç¼©ã€‚
  2. reserveTokensï¼ˆé»˜è®¤ 16384ï¼‰â€” ä¸ºè¾“å‡º/æ‘˜è¦ç”Ÿæˆé¢„ç•™çš„ token æ•°ã€‚å…·ä½“ä½œç”¨æœ‰ä¸¤å¤„ï¼š
    - è§¦å‘åˆ¤æ–­ï¼šå½“ contextTokens > contextWindow - reserveTokens æ—¶è§¦å‘ compaction
	  (ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“ä¸Šä¸‹æ–‡å¿«è¦æŠŠçª—å£å æ»¡ã€åªå‰©ä¸‹ reserveTokensçš„ç©ºé—´æ—¶ï¼Œå°±è¯¥å‹ç¼©äº†ã€‚

    - æ‘˜è¦ç”Ÿæˆé¢„ç®—ï¼šç”Ÿæˆæ‘˜è¦æ—¶ï¼ŒmaxTokens = 0.8 * reserveTokensï¼ˆä¸»æ‘˜è¦ï¼‰æˆ– 0.5 * reserveTokensï¼ˆturn prefix æ‘˜è¦ï¼‰ï¼Œç”¨ä½œ LLM è¾“å‡ºçš„ token ä¸Šé™ã€‚
	  ä¸»æ‘˜è¦å‹ç¼©çš„å†…å®¹å¤šã€ä¿¡æ¯å¯†åº¦é«˜ï¼ˆå¯èƒ½è·¨è¶Šå¤šè½®å®Œæ•´å¯¹è¯ï¼‰ï¼Œéœ€è¦æ›´å¤§çš„é¢„ç®—æ‰èƒ½ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œæ‰€ä»¥ç»™ 0.8 * reserveTokensã€‚
	  Turn prefix æ‘˜è¦åªè¦†ç›–ä¸€ä¸ª turn çš„å‰åŠæ®µï¼ŒèŒƒå›´å°å¾—å¤šï¼Œåªéœ€è¦æä¾›"å‰æƒ…æè¦"è®©åé¢ä¿ç•™çš„ suffix èƒ½è¢«ç†è§£ï¼Œæ‰€ä»¥ 0.5 * reserveTokens å°±å¤Ÿäº†ã€‚

  	  å¦å¤–ä¸¤è€…çš„é¢„ç®—åŠ èµ·æ¥æ˜¯ 1.3 * reserveTokensï¼Œè¶…è¿‡äº† reserveTokensæœ¬èº«ã€‚
	  è¿™æ²¡é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åˆ†åˆ«ç‹¬ç«‹è°ƒç”¨ LLM ç”Ÿæˆçš„ï¼ˆå„è‡ªä½œä¸º maxTokens ä¸Šé™ï¼‰ï¼Œå®é™…è¾“å‡ºé€šå¸¸è¿œå°äºä¸Šé™ï¼Œè€Œä¸”ç”Ÿæˆå®Œçš„æ‘˜è¦ä¼šæ›¿ä»£è¢«ä¸¢å¼ƒçš„å¤§é‡åŸå§‹æ¶ˆæ¯ï¼Œæœ€ç»ˆä¸Šä¸‹æ–‡ä¼šç¼©å°å¾ˆå¤šã€‚

  3. keepRecentTokensï¼ˆé»˜è®¤ 20000ï¼‰â€” compaction æ—¶ä¿ç•™æœ€è¿‘å¤šå°‘ token çš„åŸå§‹å¯¹è¯ä¸å‹ç¼©ã€‚
  	ä»å¯¹è¯æœ«å°¾å¾€å‰ç´¯åŠ  tokenï¼Œç´¯ç§¯åˆ° >= keepRecentTokens æ—¶åˆ‡ä¸€åˆ€ï¼Œåˆ‡ç‚¹ä¹‹å‰çš„æ—§æ¶ˆæ¯è¢«å‹ç¼©æˆæ‘˜è¦ï¼Œåˆ‡ç‚¹ä¹‹åçš„è¿‘æœŸæ¶ˆæ¯åŸæ ·ä¿ç•™ã€‚

  ç®€å•æ¥è¯´ï¼šreserveTokens æ§åˆ¶"ä»€ä¹ˆæ—¶å€™å‹ç¼©"å’Œ"æ‘˜è¦å¤šé•¿"ï¼ŒkeepRecentTokensæ§åˆ¶"å‹ç¼©æ—¶ä¿ç•™å¤šå°‘æœ€è¿‘çš„åŸå§‹å¯¹è¯"ã€‚
*/

export interface CompactionSettings {
	enabled: boolean;
	reserveTokens: number;
	keepRecentTokens: number;
}

export const DEFAULT_COMPACTION_SETTINGS: CompactionSettings = {
	enabled: true,
	reserveTokens: 16384,
	keepRecentTokens: 20000,
};

// ============================================================================
// Token calculation
// ============================================================================

/**
 * Calculate total context tokens from usage.
 * Uses the native totalTokens field when available, falls back to computing from components.
 */
export function calculateContextTokens(usage: Usage): number {
	return usage.totalTokens || usage.input + usage.output + usage.cacheRead + usage.cacheWrite;
}

/**
 * Get usage from an assistant message if available.
 * Skips aborted and error messages as they don't have valid usage data.
 */
function getAssistantUsage(msg: AgentMessage): Usage | undefined {
	if (msg.role === "assistant" && "usage" in msg) {
		const assistantMsg = msg as AssistantMessage;
		if (assistantMsg.stopReason !== "aborted" && assistantMsg.stopReason !== "error" && assistantMsg.usage) {
			return assistantMsg.usage;
		}
	}
	return undefined;
}

/**
 * Find the last non-aborted assistant message usage from session entries.
 */
export function getLastAssistantUsage(entries: SessionEntry[]): Usage | undefined {
	for (let i = entries.length - 1; i >= 0; i--) {
		const entry = entries[i];
		if (entry.type === "message") {
			const usage = getAssistantUsage(entry.message);
			if (usage) return usage;
		}
	}
	return undefined;
}

export interface ContextUsageEstimate {
	tokens: number;
	usageTokens: number;
	// ä¸ºä»€ä¹ˆå«  trailing?
	// å› ä¸ºè¿™äº›æ¶ˆæ¯åœ¨æœ€åä¸€æ¬¡ usage è®°å½•ä¹‹å"æ‹–å°¾"â€”â€”å°±åƒ trailing whitespaceï¼ˆå°¾éƒ¨ç©ºç™½ï¼‰ä¸€æ ·ï¼Œæ˜¯å°¾å·´ä¸Šè¿˜æ²¡è¢«ç²¾ç¡®è®¡é‡çš„é‚£éƒ¨åˆ†ã€‚
	trailingTokens: number;
	lastUsageIndex: number | null;
}

function getLastAssistantUsageInfo(messages: AgentMessage[]): { usage: Usage; index: number } | undefined {
	for (let i = messages.length - 1; i >= 0; i--) {
		const usage = getAssistantUsage(messages[i]);
		if (usage) return { usage, index: i };
	}
	return undefined;
}

/**
 * Estimate context tokens from messages, using the last assistant usage when available.
 * If there are messages after the last usage, estimate their tokens with estimateTokens.
 */
export function estimateContextTokens(messages: AgentMessage[]): ContextUsageEstimate {
	const usageInfo = getLastAssistantUsageInfo(messages);

	if (!usageInfo) {
		let estimated = 0;
		for (const message of messages) {
			estimated += estimateTokens(message);
		}
		return {
			tokens: estimated,
			usageTokens: 0,
			trailingTokens: estimated,
			lastUsageIndex: null,
		};
	}

	const usageTokens = calculateContextTokens(usageInfo.usage);
	let trailingTokens = 0;
	for (let i = usageInfo.index + 1; i < messages.length; i++) {
		trailingTokens += estimateTokens(messages[i]);
	}

	return {
		tokens: usageTokens + trailingTokens,
		usageTokens,
		trailingTokens,
		lastUsageIndex: usageInfo.index,
	};
}

/**
 * Check if compaction should trigger based on context usage.
 */
export function shouldCompact(contextTokens: number, contextWindow: number, settings: CompactionSettings): boolean {
	if (!settings.enabled) return false;
	return contextTokens > contextWindow - settings.reserveTokens;
}

// ============================================================================
// Cut point detection
// ============================================================================

/**
 * Estimate token count for a message using chars/4 heuristic.
 * This is conservative (overestimates tokens).
 */
export function estimateTokens(message: AgentMessage): number {
	/*
		ä¼°ç®—ç¡®å®æ˜¯ä¸€ä¸ªç²—ç³™çš„å¯å‘å¼æ–¹æ³•ï¼Œæ³¨é‡Šä¹Ÿæ‰¿è®¤äº†è¿™ä¸€ç‚¹ï¼ˆ"conservative, overestimates tokens"ï¼‰ã€‚

		ä¸è¿‡å®ƒåœ¨è¿™ä¸ªåœºæ™¯ä¸‹æ˜¯å¤Ÿç”¨çš„ï¼š
		- compaction çš„è§¦å‘å’Œåˆ‡ç‚¹åªéœ€è¦å¤§è‡´å‡†ç¡®ï¼Œä¸éœ€è¦ç²¾ç¡® token è®¡æ•°
		- åé«˜ä¼°ç®—ï¼ˆoverestimateï¼‰æ„å‘³ç€ä¼šæå‰è§¦å‘å‹ç¼©ã€å¤šä¿ç•™ä¸€äº›è¿‘æœŸæ¶ˆæ¯ï¼Œæ˜¯ä¸€ä¸ªå®‰å…¨æ–¹å‘çš„åå·®
		- ç”¨çœŸæ­£çš„ tokenizerï¼ˆå¦‚ tiktokenï¼‰ä¼šå¼•å…¥é¢å¤–ä¾èµ–å’Œè®¡ç®—å¼€é”€ï¼Œå¯¹äºè¿™ä¸ªç”¨é€”æ¥è¯´æ€§ä»·æ¯”ä¸é«˜
	*/

	let chars = 0;

	switch (message.role) {
		case "user": {
			const content = (message as { content: string | Array<{ type: string; text?: string }> }).content;
			if (typeof content === "string") {
				chars = content.length;
			} else if (Array.isArray(content)) {
				for (const block of content) {
					if (block.type === "text" && block.text) {
						chars += block.text.length;
					}
				}
			}
			return Math.ceil(chars / 4);
		}
		case "assistant": {
			const assistant = message as AssistantMessage;
			for (const block of assistant.content) {
				if (block.type === "text") {
					chars += block.text.length;
				} else if (block.type === "thinking") {
					chars += block.thinking.length;
				} else if (block.type === "toolCall") {
					chars += block.name.length + JSON.stringify(block.arguments).length;
				}
			}
			return Math.ceil(chars / 4);
		}
		case "custom":
		case "toolResult": {
			if (typeof message.content === "string") {
				chars = message.content.length;
			} else {
				for (const block of message.content) {
					if (block.type === "text" && block.text) {
						chars += block.text.length;
					}
					if (block.type === "image") {
						chars += 4800; // Estimate images as 4000 chars, or 1200 tokens
					}
				}
			}
			return Math.ceil(chars / 4);
		}
		case "bashExecution": {
			chars = message.command.length + message.output.length;
			return Math.ceil(chars / 4);
		}
		case "branchSummary":
		case "compactionSummary": {
			chars = message.summary.length;
			return Math.ceil(chars / 4);
		}
	}

	return 0;
}

/**
 * Find valid cut points: indices of user, assistant, custom, or bashExecution messages.
 * Never cut at tool results (they must follow their tool call).
 * When we cut at an assistant message with tool calls, its tool results follow it
 * and will be kept.
 * BashExecutionMessage is treated like a user message (user-initiated context).
 */
function findValidCutPoints(entries: SessionEntry[], startIndex: number, endIndex: number): number[] {
	/*
		åœ¨ç»™å®šèŒƒå›´å†…æ‰¾å‡ºæ‰€æœ‰å¯ä»¥ä½œä¸ºåˆ‡ç‚¹çš„ entry ç´¢å¼•ã€‚

		æ ¸å¿ƒè§„åˆ™ï¼šæ°¸è¿œä¸åœ¨ toolResult å¤„åˆ‡ã€‚
		å› ä¸º toolResult å¿…é¡»ç´§è·Ÿåœ¨å®ƒå¯¹åº”çš„ tool call (assistant æ¶ˆæ¯) ä¹‹åï¼Œå¦‚æœåœ¨ toolResult å¤„åˆ‡æ–­ï¼Œå°±ä¼šæŠŠå·¥å…·è°ƒç”¨å’Œå·¥å…·ç»“æœæ‹†æ•£ï¼Œç ´åå¯¹è¯ç»“æ„ã€‚
		åˆæ³•åˆ‡ç‚¹åŒ…æ‹¬ï¼š
		- message ç±»å‹ä¸­çš„ userã€assistantã€bashExecutionã€customã€branchSummaryã€compactionSummary
		- entry ç±»å‹ä¸º branch_summary æˆ– custom_message

		å…¶ä½™ entry ç±»å‹ï¼ˆthinking_level_changeã€model_changeã€compactionã€labelç­‰ï¼‰æ˜¯å…ƒæ•°æ®ï¼Œä¸ç®—åˆæ³•åˆ‡ç‚¹ï¼Œç›´æ¥è·³è¿‡ï¼ˆswitch é‡Œ fall through åˆ°ç©ºå¤„ç†ï¼‰ã€‚
		è¿”å›çš„ç´¢å¼•æ•°ç»„ä¾› findCutPoint ä½¿ç”¨ï¼Œåœ¨è¿™äº›åˆæ³•ä½ç½®ä¸­æŒ‰ keepRecentTokens é€‰æ‹©æœ€ç»ˆåˆ‡ç‚¹ã€‚
	*/
	const cutPoints: number[] = [];
	for (let i = startIndex; i < endIndex; i++) {
		const entry = entries[i];
		switch (entry.type) {
			case "message": {
				const role = entry.message.role;
				switch (role) {
					case "bashExecution":
					case "custom":
					case "branchSummary":
					case "compactionSummary":
					case "user":
					case "assistant":
						cutPoints.push(i);
						break;
					case "toolResult":
						break;
				}
				break;
			}
			case "thinking_level_change":
			case "model_change":
			case "compaction":
			case "branch_summary":
			case "custom":
			case "custom_message":
			case "label":
		}
		// branch_summary and custom_message are user-role messages, valid cut points
		if (entry.type === "branch_summary" || entry.type === "custom_message") {
			cutPoints.push(i);
		}
	}
	return cutPoints;
}

/**
 * Find the user message (or bashExecution) that starts the turn containing the given entry index.
 * Returns -1 if no turn start found before the index.
 * BashExecutionMessage is treated like a user message for turn boundaries.
 */
export function findTurnStartIndex(entries: SessionEntry[], entryIndex: number, startIndex: number): number {
	for (let i = entryIndex; i >= startIndex; i--) {
		const entry = entries[i];
		// branch_summary and custom_message are user-role messages, can start a turn
		if (entry.type === "branch_summary" || entry.type === "custom_message") {
			return i;
		}
		if (entry.type === "message") {
			const role = entry.message.role;
			if (role === "user" || role === "bashExecution") {
				return i;
			}
		}
	}
	return -1;
}

export interface CutPointResult {
	/** Index of first entry to keep */
	// åˆ‡ç‚¹ä½ç½®ï¼Œä»è¿™ä¸ªç´¢å¼•å¼€å§‹çš„ entry ä¼šè¢«åŸæ ·ä¿ç•™ï¼Œä¹‹å‰çš„è¢«å‹ç¼©
	firstKeptEntryIndex: number;
	/** Index of user message that starts the turn being split, or -1 if not splitting */
	// å¦‚æœåˆ‡ç‚¹è½åœ¨ä¸€ä¸ª turn ä¸­é—´ï¼Œè¿™æ˜¯è¯¥ turn èµ·å§‹ user æ¶ˆæ¯çš„ç´¢å¼•ï¼›æ²¡æœ‰åˆ‡åˆ† turn æ—¶ä¸º -1
	turnStartIndex: number;
	/** Whether this cut splits a turn (cut point is not a user message) */
	// åˆ‡ç‚¹æ˜¯å¦æŠŠä¸€ä¸ª turn åŠˆæˆäº†ä¸¤åŠï¼ˆå³ firstKeptEntryIndex ä¸æ˜¯ä¸€ä¸ª user æ¶ˆæ¯ï¼Œè€Œæ˜¯ turn ä¸­é—´çš„æŸæ¡ assistant/tool æ¶ˆæ¯ï¼‰
	// å½“ isSplitTurn === true æ—¶ï¼Œ[turnStartIndex, firstKeptEntryIndex) è¿™æ®µå°±æ˜¯éœ€è¦å•ç‹¬ç”Ÿæˆ turn prefix æ‘˜è¦çš„éƒ¨åˆ†ã€‚
	isSplitTurn: boolean;
}

/**
 * Find the cut point in session entries that keeps approximately `keepRecentTokens`.
 *
 * Algorithm: Walk backwards from newest, accumulating estimated message sizes.
 * Stop when we've accumulated >= keepRecentTokens. Cut at that point.
 *
 * Can cut at user OR assistant messages (never tool results). When cutting at an
 * assistant message with tool calls, its tool results come after and will be kept.
 *
 * Returns CutPointResult with:
 * - firstKeptEntryIndex: the entry index to start keeping from
 * - turnStartIndex: if cutting mid-turn, the user message that started that turn
 * - isSplitTurn: whether we're cutting in the middle of a turn
 *
 * Only considers entries between `startIndex` and `endIndex` (exclusive).
 */
export function findCutPoint(
	entries: SessionEntry[],
	startIndex: number,
	endIndex: number,
	keepRecentTokens: number,
): CutPointResult {
	const cutPoints = findValidCutPoints(entries, startIndex, endIndex);

	if (cutPoints.length === 0) {
		return { firstKeptEntryIndex: startIndex, turnStartIndex: -1, isSplitTurn: false };
	}

	// Walk backwards from newest, accumulating estimated message sizes
	let accumulatedTokens = 0;
	let cutIndex = cutPoints[0]; // Default: keep from first message (not header)

	for (let i = endIndex - 1; i >= startIndex; i--) {
		const entry = entries[i];
		// åœ¨ä»åå¾€å‰ç´¯åŠ  token æ—¶ï¼Œåªç»Ÿè®¡ message ç±»å‹çš„ entryã€‚
		// å…¶ä»–ç±»å‹ï¼ˆthinking_level_changeã€model_changeã€label ç­‰ï¼‰æ˜¯å…ƒæ•°æ®ï¼Œä¸å  LLM ä¸Šä¸‹æ–‡ tokenï¼Œæ‰€ä»¥è·³è¿‡ä¸è®¡ã€‚
		if (entry.type !== "message") continue;

		// Estimate this message's size
		const messageTokens = estimateTokens(entry.message);
		accumulatedTokens += messageTokens;

		// Check if we've exceeded the budget
		if (accumulatedTokens >= keepRecentTokens) {
			// Find the closest valid cut point at or after this entry
			for (let c = 0; c < cutPoints.length; c++) {
				if (cutPoints[c] >= i) {
					cutIndex = cutPoints[c];
					break;
				}
			}
			break;
		}
	}

	/*
		ç¡®å®šåˆ‡ç‚¹åï¼Œå¾€å‰å¤šæä¸€äº›ç´§æŒ¨ç€åˆ‡ç‚¹çš„éæ¶ˆæ¯ entryã€‚
		æ¯”å¦‚åˆ‡ç‚¹å‰é¢å¯èƒ½æœ‰ thinking_level_changeã€model_changeã€label è¿™ç±»å…ƒæ•°æ® entryã€‚å®ƒä»¬ä¸å  token ä½†å¯èƒ½æ˜¯ç´§éšå…¶åæ¶ˆæ¯çš„é…ç½®ä¸Šä¸‹æ–‡ï¼ˆæ¯”å¦‚åœ¨æŸæ¡æ¶ˆæ¯å‰åˆ‡æ¢äº†æ¨¡å‹ï¼‰ã€‚
		å¦‚æœä¸æŠŠå®ƒä»¬ä¸€èµ·ä¿ç•™ï¼Œä¿ç•™çš„æ¶ˆæ¯å¯èƒ½ä¼šç¼ºå°‘æ­£ç¡®çš„é…ç½®çŠ¶æ€ã€‚
		å¾€å‰æ‰«æçš„åœæ­¢æ¡ä»¶ï¼šç¢°åˆ° compactionï¼ˆä¸Šæ¬¡å‹ç¼©è¾¹ç•Œï¼‰æˆ– messageï¼ˆå±äºæ›´æ—©çš„å¯¹è¯å†…å®¹ï¼‰å°±åœã€‚
		æ•ˆæœï¼šåˆ‡ç‚¹ä»"ç¬¬ä¸€æ¡ä¿ç•™çš„ message"å‰ç§»åˆ°"ç¬¬ä¸€æ¡ä¿ç•™çš„ message åŠå…¶å‰é¢ç´§é‚»çš„å…ƒæ•°æ® entry"ã€‚
	*/

	// Scan backwards from cutIndex to include any non-message entries (bash, settings, etc.)
	while (cutIndex > startIndex) {
		const prevEntry = entries[cutIndex - 1];
		// Stop at session header or compaction boundaries
		if (prevEntry.type === "compaction") {
			break;
		}
		if (prevEntry.type === "message") {
			// Stop if we hit any message
			break;
		}
		// Include this non-message entry (bash, settings change, etc.)
		cutIndex--;
	}

	/*
		åœ¨ findCutPoint ä¸­ï¼Œå½“åˆ‡ç‚¹è½åœ¨ assistant æ¶ˆæ¯ä¸Šæ—¶ï¼Œç”¨è¿™ä¸ªå‡½æ•°æ‰¾åˆ°è¯¥ turn çš„ user æ¶ˆæ¯ä½ç½®ï¼Œ
		ä»è€Œç¡®å®š turnStartIndexï¼ŒçŸ¥é“ turn prefix çš„èŒƒå›´æ˜¯ [turnStartIndex, firstKeptEntryIndex)ã€‚
	*/

	// Determine if this is a split turn
	const cutEntry = entries[cutIndex];
	const isUserMessage = cutEntry.type === "message" && cutEntry.message.role === "user";
	const turnStartIndex = isUserMessage ? -1 : findTurnStartIndex(entries, cutIndex, startIndex);

	return {
		firstKeptEntryIndex: cutIndex,
		turnStartIndex,
		isSplitTurn: !isUserMessage && turnStartIndex !== -1,
	};
}

// ============================================================================
// Summarization
// ============================================================================

/*
ä¸»æ‘˜è¦ (SUMMARIZATION_PROMPT / UPDATE_SUMMARIZATION_PROMPT)
  - å‹ç¼©å¯¹è±¡ï¼šåˆ‡ç‚¹ä¹‹å‰çš„å®Œæ•´å†å² turnï¼ˆå¯èƒ½è·¨è¶Šå¤šè½®å¯¹è¯ï¼‰
  - token é¢„ç®—ï¼š0.8 * reserveTokensï¼ˆè¾ƒå¤§ï¼‰
  - æ ¼å¼ï¼šç»“æ„åŒ–çš„"é¡¹ç›®æ£€æŸ¥ç‚¹"ï¼ŒåŒ…å« 6 ä¸ªå›ºå®š sectionï¼š
    - Goal / Constraints & Preferences / Progress (Done/In Progress/Blocked) / Key Decisions / Next Steps / Critical Context
  - å¢é‡æ¨¡å¼ï¼šå¦‚æœå·²æœ‰ä¸Šä¸€æ¬¡ compaction çš„æ‘˜è¦(previousSummary)ï¼Œä¼šç”¨ UPDATE_SUMMARIZATION_PROMPT åšå¢é‡æ›´æ–°è€Œéé‡å†™ï¼Œä¿ç•™å·²æœ‰ä¿¡æ¯å¹¶åˆå…¥æ–°å†…å®¹
*/

const SUMMARIZATION_PROMPT = `The messages above are a conversation to summarize. Create a structured context checkpoint summary that another LLM will use to continue the work.

Use this EXACT format:

## Goal
[What is the user trying to accomplish? Can be multiple items if the session covers different tasks.]

## Constraints & Preferences
- [Any constraints, preferences, or requirements mentioned by user]
- [Or "(none)" if none were mentioned]

## Progress
### Done
- [x] [Completed tasks/changes]

### In Progress
- [ ] [Current work]

### Blocked
- [Issues preventing progress, if any]

## Key Decisions
- **[Decision]**: [Brief rationale]

## Next Steps
1. [Ordered list of what should happen next]

## Critical Context
- [Any data, examples, or references needed to continue]
- [Or "(none)" if not applicable]

Keep each section concise. Preserve exact file paths, function names, and error messages.`;

const UPDATE_SUMMARIZATION_PROMPT = `The messages above are NEW conversation messages to incorporate into the existing summary provided in <previous-summary> tags.

Update the existing structured summary with new information. RULES:
- PRESERVE all existing information from the previous summary
- ADD new progress, decisions, and context from the new messages
- UPDATE the Progress section: move items from "In Progress" to "Done" when completed
- UPDATE "Next Steps" based on what was accomplished
- PRESERVE exact file paths, function names, and error messages
- If something is no longer relevant, you may remove it

Use this EXACT format:

## Goal
[Preserve existing goals, add new ones if the task expanded]

## Constraints & Preferences
- [Preserve existing, add new ones discovered]

## Progress
### Done
- [x] [Include previously done items AND newly completed items]

### In Progress
- [ ] [Current work - update based on progress]

### Blocked
- [Current blockers - remove if resolved]

## Key Decisions
- **[Decision]**: [Brief rationale] (preserve all previous, add new)

## Next Steps
1. [Update based on current state]

## Critical Context
- [Preserve important context, add new if needed]

Keep each section concise. Preserve exact file paths, function names, and error messages.`;

/**
 * Generate a summary of the conversation using the LLM.
 * If previousSummary is provided, uses the update prompt to merge.
 */
export async function generateSummary(
	currentMessages: AgentMessage[],
	model: Model<any>,
	reserveTokens: number,
	apiKey: string,
	signal?: AbortSignal,
	customInstructions?: string,
	previousSummary?: string,
): Promise<string> {
	const maxTokens = Math.floor(0.8 * reserveTokens);

	// Use update prompt if we have a previous summary, otherwise initial prompt
	let basePrompt = previousSummary ? UPDATE_SUMMARIZATION_PROMPT : SUMMARIZATION_PROMPT;
	if (customInstructions) {
		basePrompt = `${basePrompt}\n\nAdditional focus: ${customInstructions}`;
	}

	// Serialize conversation to text so model doesn't try to continue it
	// Convert to LLM messages first (handles custom types like bashExecution, custom, etc.)
	const llmMessages = convertToLlm(currentMessages);
	const conversationText = serializeConversation(llmMessages);

	// Build the prompt with conversation wrapped in tags
	let promptText = `<conversation>\n${conversationText}\n</conversation>\n\n`;
	if (previousSummary) {
		promptText += `<previous-summary>\n${previousSummary}\n</previous-summary>\n\n`;
	}
	promptText += basePrompt;

	const summarizationMessages = [
		{
			role: "user" as const,
			content: [{ type: "text" as const, text: promptText }],
			timestamp: Date.now(),
		},
	];

	const response = await completeSimple(
		model,
		{ systemPrompt: SUMMARIZATION_SYSTEM_PROMPT, messages: summarizationMessages },
		{ maxTokens, signal, apiKey, reasoning: "high" },
	);

	if (response.stopReason === "error") {
		throw new Error(`Summarization failed: ${response.errorMessage || "Unknown error"}`);
	}

	const textContent = response.content
		.filter((c): c is { type: "text"; text: string } => c.type === "text")
		.map((c) => c.text)
		.join("\n");

	return textContent;
}

// ============================================================================
// Compaction Preparation (for extensions)
// ============================================================================

export interface CompactionPreparation {
	/** UUID of first entry to keep */
	firstKeptEntryId: string;
	/** Messages that will be summarized and discarded */
	messagesToSummarize: AgentMessage[];
	/** Messages that will be turned into turn prefix summary (if splitting) */
	turnPrefixMessages: AgentMessage[];
	/** Whether this is a split turn (cut point in middle of turn) */
	isSplitTurn: boolean;
	tokensBefore: number;
	/** Summary from previous compaction, for iterative update */
	previousSummary?: string;
	/** File operations extracted from messagesToSummarize */
	fileOps: FileOperations;
	/** Compaction settions from settings.jsonl	*/
	settings: CompactionSettings;
}

export function prepareCompaction(
	pathEntries: SessionEntry[],
	settings: CompactionSettings,
): CompactionPreparation | undefined {
	// å¦‚æœå¯¹è¯çš„æœ€åä¸€æ¡ entry å°±æ˜¯ compaction ç±»å‹ï¼ˆå³ä¸Šæ¬¡ compactionä¹‹åæ²¡æœ‰ä»»ä½•æ–°æ¶ˆæ¯ï¼‰ï¼Œå°±ç›´æ¥è¿”å› undefined è¡¨ç¤ºä¸éœ€è¦å†å‹ç¼©ã€‚
	// é˜²æ­¢å¯¹ä¸€ä¸ªåˆšå‹ç¼©å®Œã€è¿˜æ²¡æœ‰æ–°å¯¹è¯å†…å®¹çš„ session é‡å¤å‹ç¼©ã€‚
	if (pathEntries.length > 0 && pathEntries[pathEntries.length - 1].type === "compaction") {
		return undefined;
	}

	// ä»åå¾€å‰æ‰¾åˆ°æœ€è¿‘ä¸€æ¬¡ compaction entry çš„ä½ç½®
	let prevCompactionIndex = -1;
	for (let i = pathEntries.length - 1; i >= 0; i--) {
		if (pathEntries[i].type === "compaction") {
			prevCompactionIndex = i;
			break;
		}
	}

	const boundaryStart = prevCompactionIndex + 1;
	const boundaryEnd = pathEntries.length;

	const usageStart = prevCompactionIndex >= 0 ? prevCompactionIndex : 0;
	const usageMessages: AgentMessage[] = [];
	for (let i = usageStart; i < boundaryEnd; i++) {
		const msg = getMessageFromEntry(pathEntries[i]);
		if (msg) usageMessages.push(msg);
	}
	const tokensBefore = estimateContextTokens(usageMessages).tokens;

	const cutPoint = findCutPoint(pathEntries, boundaryStart, boundaryEnd, settings.keepRecentTokens);

	// Get UUID of first kept entry
	const firstKeptEntry = pathEntries[cutPoint.firstKeptEntryIndex];
	if (!firstKeptEntry?.id) {
		return undefined; // Session needs migration
	}
	const firstKeptEntryId = firstKeptEntry.id;

	const historyEnd = cutPoint.isSplitTurn ? cutPoint.turnStartIndex : cutPoint.firstKeptEntryIndex;

	// Messages to summarize (will be discarded after summary)
	const messagesToSummarize: AgentMessage[] = [];
	for (let i = boundaryStart; i < historyEnd; i++) {
		const msg = getMessageFromEntry(pathEntries[i]);
		if (msg) messagesToSummarize.push(msg);
	}

	// Messages for turn prefix summary (if splitting a turn)
	const turnPrefixMessages: AgentMessage[] = [];
	if (cutPoint.isSplitTurn) {
		for (let i = cutPoint.turnStartIndex; i < cutPoint.firstKeptEntryIndex; i++) {
			const msg = getMessageFromEntry(pathEntries[i]);
			if (msg) turnPrefixMessages.push(msg);
		}
	}

	// Get previous summary for iterative update
	let previousSummary: string | undefined;
	if (prevCompactionIndex >= 0) {
		const prevCompaction = pathEntries[prevCompactionIndex] as CompactionEntry;
		previousSummary = prevCompaction.summary;
	}

	// Extract file operations from messages and previous compaction
	const fileOps = extractFileOperations(messagesToSummarize, pathEntries, prevCompactionIndex);

	// Also extract file ops from turn prefix if splitting
	if (cutPoint.isSplitTurn) {
		for (const msg of turnPrefixMessages) {
			extractFileOpsFromMessage(msg, fileOps);
		}
	}

	// ğŸ‘‡ è¿™ä¸ªç»“æ„æœ¬è´¨ä¸Šæ˜¯æŠŠ compaction æ‹†æˆäº†å‡†å¤‡é˜¶æ®µï¼ˆçº¯è®¡ç®—ï¼Œç¡®å®šåˆ‡ç‚¹å’Œåˆ†ç»„æ¶ˆæ¯ï¼‰å’Œæ‰§è¡Œé˜¶æ®µï¼ˆè°ƒ LLMç”Ÿæˆæ‘˜è¦ï¼‰ï¼Œä¸­é—´æš´éœ²ç»™ extension ä¸€ä¸ªä»‹å…¥æœºä¼šã€‚
	return {
		// åˆ‡ç‚¹å¤„ç¬¬ä¸€æ¡ä¿ç•™ entry çš„ UUID
		firstKeptEntryId,
		// åˆ‡ç‚¹ä¹‹å‰çš„å®Œæ•´ turn æ¶ˆæ¯ï¼Œä¼šè¢«å‹ç¼©æˆä¸»æ‘˜è¦ç„¶åä¸¢å¼ƒ
		messagesToSummarize,
		// å¦‚æœåˆ‡åˆ†äº†ä¸€ä¸ª turnï¼Œè¿™æ˜¯è¢«åˆ‡æ‰çš„å‰åŠæ®µæ¶ˆæ¯ï¼Œç”¨äºç”Ÿæˆ turn prefix æ‘˜è¦ï¼›æ²¡æœ‰åˆ‡åˆ†æ—¶ä¸ºç©ºæ•°ç»„
		turnPrefixMessages,
		// åˆ‡ç‚¹æ˜¯å¦æŠŠæŸä¸ª turn åŠˆæˆäº†ä¸¤åŠ
		isSplitTurn: cutPoint.isSplitTurn,
		//  compaction å‰çš„ä¸Šä¸‹æ–‡ token ä¼°ç®—å€¼ï¼Œä¼šè®°å½•åˆ° CompactionEntry ä¸­ï¼Œç”¨äºå¯¹æ¯”å‹ç¼©æ•ˆæœ
		tokensBefore,
		// ä¸Šæ¬¡ compaction çš„æ‘˜è¦æ–‡æœ¬ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼Œä¼ ç»™ generateSummary åšå¢é‡æ›´æ–°è€Œéé‡å†™
		previousSummary,
		// ä»å¾…å‹ç¼©æ¶ˆæ¯å’Œä¸Šæ¬¡ compaction details ä¸­æå–çš„æ–‡ä»¶æ“ä½œè®°å½•ï¼ˆread/editedï¼‰ï¼Œè¿½åŠ åˆ°æ‘˜è¦æœ«å°¾
		fileOps,
		// å½“å‰çš„ compaction é…ç½®ï¼Œä¼ é€’ç»™åç»­çš„ compact() ä½¿ç”¨
		settings,
	};
}

// ============================================================================
// Main compaction function
// ============================================================================

const TURN_PREFIX_SUMMARIZATION_PROMPT = `This is the PREFIX of a turn that was too large to keep. The SUFFIX (recent work) is retained.

Summarize the prefix to provide context for the retained suffix:

## Original Request
[What did the user ask for in this turn?]

## Early Progress
- [Key decisions and work done in the prefix]

## Context for Suffix
- [Information needed to understand the retained recent work]

Be concise. Focus on what's needed to understand the kept suffix.`;

/**
 * Generate summaries for compaction using prepared data.
 * Returns CompactionResult - SessionManager adds uuid/parentUuid when saving.
 *
 * @param preparation - Pre-calculated preparation from prepareCompaction()
 * @param customInstructions - Optional custom focus for the summary
 */
export async function compact(
	preparation: CompactionPreparation,
	model: Model<any>,
	apiKey: string,
	customInstructions?: string,
	signal?: AbortSignal,
): Promise<CompactionResult> {
	const {
		firstKeptEntryId,
		messagesToSummarize,
		turnPrefixMessages,
		isSplitTurn,
		tokensBefore,
		previousSummary,
		fileOps,
		settings,
	} = preparation;

	// Generate summaries (can be parallel if both needed) and merge into one
	let summary: string;

	if (isSplitTurn && turnPrefixMessages.length > 0) {
		// Generate both summaries in parallel
		const [historyResult, turnPrefixResult] = await Promise.all([
			messagesToSummarize.length > 0
				? generateSummary(
						messagesToSummarize,
						model,
						settings.reserveTokens,
						apiKey,
						signal,
						customInstructions,
						previousSummary,
					)
				: Promise.resolve("No prior history."),
			generateTurnPrefixSummary(turnPrefixMessages, model, settings.reserveTokens, apiKey, signal),
		]);
		// Merge into single summary
		summary = `${historyResult}\n\n---\n\n**Turn Context (split turn):**\n\n${turnPrefixResult}`;
	} else {
		// Just generate history summary
		summary = await generateSummary(
			messagesToSummarize,
			model,
			settings.reserveTokens,
			apiKey,
			signal,
			customInstructions,
			previousSummary,
		);
	}

	// Compute file lists and append to summary
	const { readFiles, modifiedFiles } = computeFileLists(fileOps);
	summary += formatFileOperations(readFiles, modifiedFiles);

	if (!firstKeptEntryId) {
		throw new Error("First kept entry has no UUID - session may need migration");
	}

	return {
		summary,
		firstKeptEntryId,
		tokensBefore,
		details: { readFiles, modifiedFiles } as CompactionDetails,
	};
}

/**
 * Generate a summary for a turn prefix (when splitting a turn).
 */
async function generateTurnPrefixSummary(
	messages: AgentMessage[],
	model: Model<any>,
	reserveTokens: number,
	apiKey: string,
	signal?: AbortSignal,
): Promise<string> {
	const maxTokens = Math.floor(0.5 * reserveTokens); // Smaller budget for turn prefix
	const llmMessages = convertToLlm(messages);
	const conversationText = serializeConversation(llmMessages);
	const promptText = `<conversation>\n${conversationText}\n</conversation>\n\n${TURN_PREFIX_SUMMARIZATION_PROMPT}`;
	const summarizationMessages = [
		{
			role: "user" as const,
			content: [{ type: "text" as const, text: promptText }],
			timestamp: Date.now(),
		},
	];

	const response = await completeSimple(
		model,
		{ systemPrompt: SUMMARIZATION_SYSTEM_PROMPT, messages: summarizationMessages },
		{ maxTokens, signal, apiKey },
	);

	if (response.stopReason === "error") {
		throw new Error(`Turn prefix summarization failed: ${response.errorMessage || "Unknown error"}`);
	}

	return response.content
		.filter((c): c is { type: "text"; text: string } => c.type === "text")
		.map((c) => c.text)
		.join("\n");
}
